---
title: "STAT project"
output: pdf_document
---

## About Dataset

## Longitudinal MRI Data in Nondemented and Demented Older Adults: This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer's disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.

## Actuarial Life Table: A period life table is based on the mortality experience of a population during a relatively short period of time. Here we present the 2020 period life table for the Social Security area population, as used in the 2023 Trustees Report (TR). For this table, the period life expectancy at a given age is the average remaining number of years expected prior to death for a person at that exact age, born on January 1, using the mortality rates for 2020 over the course of his or her remaining life.

```{r, echo=TRUE,warning=FALSE,message=FALSE}
library(tidyverse)
library(tidymodels)
library(eeptools)
library(rvest)

```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##Scrape  data from online
alz=read_csv("C:/Users/endkg/Desktop/oasis_longitudinal.csv")
urls=read_html("https://www.ssa.gov/oact/STATS/table4c6.html")
tablelifeexp=urls %>% html_table(fill=TRUE)
exp=tablelifeexp %>% as.data.frame() 
exptable=exp[-1,] %>% rename(age=Exact.age,male_lifeexp=Male.2,female_lifeexp=Female.2) %>% select(age,male_lifeexp,female_lifeexp) %>% as.tibble() %>% mutate_if(is.character,as.double)
alz_lifeexp=alz %>% left_join(exptable,by=c("Age"="age")) %>% rename(sex=`M/F`)
alz_lifeexp=alz_lifeexp %>% mutate_at(c("Hand","Group","sex"),as.factor)
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
## Data Exploration
library(PerformanceAnalytics)
alz_lifeexp %>% select(Age,sex) %>% ggplot(aes(Age,fill=sex))+
  geom_histogram()+
  facet_wrap(~sex)## Age by sex graph

chart.Correlation(select(alz_lifeexp,Age,EDUC,SES,MMSE,CDR,eTIV,nWBV,ASF),
                  histogram = TRUE,main="correlation") ##all variable correlation 
alz_lifeexp %>% select(eTIV,ASF) %>% ggplot(aes(x=eTIV,y=ASF))+geom_point(color="red")+
geom_smooth() ## significant negative correlation between ASF and eTIV. 

alz_lifeexp %>% select(CDR,EDUC,sex) %>% ggplot(aes(x=CDR,y=EDUC,fill=sex))+
  geom_boxplot() ##degree CDR by education 

alz_lifeexp %>% select(Group,sex,CDR) %>% ggplot(aes(x=Group,fill=as.factor(CDR),
                                                       color=sex))+
  geom_bar()## count group by CDR and sex
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##split data
library(tidymodels)
library(modelr)
library(recipes)
set.seed(1)
alzsplit=alz_lifeexp %>% initial_split(prop = 0.8)
alztrain=alzsplit %>% training() 
alztest=alzsplit %>% testing()
alzdata=recipe(CDR~sex+Age+SES+EDUC+MMSE+eTIV+nWBV,data=alztrain)
alzdata=alzdata %>% step_impute_mean(SES)
alzdata=alzdata %>% step_impute_mean(MMSE)
alzdata=alzdata %>% step_dummy(all_nominal_predictors())
alzdata=alzdata %>% step_scale(all_nominal_predictors())
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##lasso
library(glmnet)
set.seed(1)
recipeworkflow=workflow()
alzlasso=linear_reg(penalty = tune(),mixture=1) %>% set_engine("glmnet") 
lassolambda=grid_regular(penalty(),levels = 3)
lassogrid=tune_grid(recipeworkflow %>% add_recipe(alzdata) %>% 
add_model(alzlasso),grid=lassolambda,resamples=bootstraps(alztrain))
lassogrid %>% collect_metrics()
lowest_rmse=lassogrid %>% select_best("rmse",maximize=FALSE)
final_lasso=finalize_workflow(recipeworkflow %>% add_recipe(alzdata) %>%
add_model(alzlasso),lowest_rmse)
lasso_alz_fit=final_lasso %>% fit(alztrain)
lasso_alz_pred=lasso_alz_fit %>% predict(alztest)
lassox2=data.matrix(alztest[,c('sex','Age','SES','EDUC','MMSE','eTIV','nWBV')])
lassoy2=alztest$CDR
sst=sum((lassoy2-mean(lassoy2))^2)
sse=sum((lasso_alz_pred-lassoy2)^2)
rs=1-sse/sst
rs
rsq_trad(lasso_alz_pred,truth = lassoy2,estimate = as.numeric(unlist(lasso_alz_pred)))
##only 47.1 percent fit, maybe lasso is not a good model

##ridge regression

alzridge=linear_reg(penalty = tune(),mixture=0) %>% set_engine("glmnet") 
ridgelambda=grid_regular(penalty(),levels = 3)
ridgegrid=tune_grid(recipeworkflow %>% add_recipe(alzdata) %>%
add_model(alzridge),grid=ridgelambda,resamples=bootstraps(alztrain))
ridgegrid %>% collect_metrics()
lowest_rmse2=ridgegrid %>% select_best("rmse",maximize=FALSE)
final_ridge=finalize_workflow(recipeworkflow %>% add_recipe(alzdata) %>%
add_model(alzridge),lowest_rmse2)
ridge_alz_fit=final_ridge %>% fit(alztrain)
ridge_alz_pred=ridge_alz_fit %>% predict(alztest)
ridgex2=data.matrix(alztest[,c('sex','Age','SES','EDUC','MMSE','eTIV','nWBV')])
ridgey2=alztest$CDR
sst=sum((ridgey2-mean(ridgey2))^2)
sse=sum((ridge_alz_pred-ridgey2)^2)
rsridge=1-sse/sst
rsridge
rsq_trad(ridge_alz_pred,truth = ridgey2,estimate = as.numeric(unlist(ridge_alz_pred)))
## ridge regression also only fit 46.4 percent, may not a good model,
##but better than lasso.


##lm

recipeworkflow=workflow()
alzlm=linear_reg() %>% set_engine("lm") %>% set_mode("regression") 
recipeworkflowlm=recipeworkflow %>% add_model(alzlm) %>% add_recipe(alzdata)
lm_alz_fit=recipeworkflowlm %>% fit(alztrain)
lm_alz_pred=lm_alz_fit %>% predict(alztest)
lmx2=data.matrix(alztest[,c('sex','Age','SES','EDUC','MMSE','eTIV','nWBV')])
lmy2=alztest$CDR
sstlm=sum((lmy2-mean(lmy2))^2)
sselm=sum((lm_alz_pred-lmy2)^2)
rslm=1-sselm/sstlm
rslm## lm has only fit 47.2 percent, which worse than lasso and ridge regression.
rsq_trad(lm_alz_pred,truth = lmy2,estimate = as.numeric(unlist(lm_alz_pred)))
##knn
library(kknn)
knnalz=nearest_neighbor() %>% set_mode("regression") %>% 
set_engine("kknn",neighbors=5)
recipeworkflowknn=recipeworkflow %>% add_model(knnalz) %>% add_recipe(alzdata)
knn_alz_fit=recipeworkflowknn %>% fit(alztrain)
knn_alz_pred=knn_alz_fit %>% predict(alztest)
knnx2=data.matrix(alztest[,c('sex','Age','SES','EDUC','MMSE','eTIV','nWBV')])
knny2=alztest$CDR
sstknn=sum((knny2-mean(knny2))^2)
sseknn=sum((knn_alz_pred-knny2)^2)
rsknn=1-sseknn/sstknn
rsknn
rsq_trad(knn_alz_pred,truth = knny2,estimate = as.numeric(unlist(knn_alz_pred)))
## knn model fit 59.3 percent, which is the better than lasso,ridge and linear model. 

##random forest
library(ranger)
set.seed(1)
rfalz=rand_forest() %>% set_mode("regression") %>% set_engine("ranger")
recipeworkflowrf=recipeworkflow %>% add_model(rfalz) %>% add_recipe(alzdata)
rf_alz_fit=recipeworkflowrf %>% fit(alztrain)
rf_alz_pred=rf_alz_fit %>% predict(alztest)
rfx2=data.matrix(alztest[,c('sex','Age','SES','EDUC','MMSE','eTIV','nWBV')])
rfy2=alztest$CDR
sstrf=sum((rfy2-mean(rfy2))^2)
sserf=sum((rf_alz_pred-rfy2)^2)
rsrf=1-sserf/sstrf
rsrf
rsq_trad(rf_alz_pred,truth = rfy2,estimate = as.numeric(unlist(rf_alz_pred)))
## only 53.2 percent to fit the model.
## After all, base on these 5 output, knn is the best model.
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##bootstrap
setwd("C:/Users/endkg/Desktop")
set.seed(1)
goal_metrics=metric_set(yardstick::rmse,yardstick::rsq_trad,
                                   yardstick::mae,rsq)

alz_bootstrap=alztrain %>% bootstraps(times=100)
bootstrap_knnflow=workflow() %>% add_model(knnalz) %>% add_recipe(alzdata)
#bootstrap_knnfit=bootstrap_knnflow %>% fit_resamples(alz_bootstrap,metrics=goal_metrics)
#save(bootstrap_knnfit,file="bootstrap_knnfit.rda")
load(file="bootstrap_knnfit.rda")

#bootstrap_lassofit=final_lasso %>% 
#fit_resamples(alz_bootstrap,metrics=goal_metrics)
#save(bootstrap_lassofit,file="bootstrap_lassofit.rda")
load(file="bootstrap_lassofit.rda")

#bootstrap_ridgefit=final_ridge %>% 
#fit_resamples(alz_bootstrap,metrics=goal_metrics)
#save(bootstrap_ridgefit,file='bootstrap_ridgefit.rda')
load(file="bootstrap_ridgefit.rda")

bootstrap_lmflow=workflow() %>% add_model(alzlm) %>% add_recipe(alzdata)
#bootstrap_lmfit=bootstrap_lmflow %>% fit_resamples(alz_bootstrap,metrics=goal_metrics)
#save(bootstrap_lmfit,file="bootstrap_lmfit.rda")
load(file="bootstrap_lmfit.rda")

bootstrap_rfflow=workflow() %>% add_model(rfalz) %>% add_recipe(alzdata)
#bootstrap_rffit=bootstrap_rfflow %>% fit_resamples(alz_bootstrap,metrics=goal_metrics)
#save(bootstrap_rffit,file='bootstrap_rffit.rda')
load(file="bootstrap_rffit.rda")

performanceknn=bootstrap_knnfit %>% collect_metrics(summarize = FALSE) %>% 
select(id,.metric,.estimate) %>% mutate(model="knn")
performancelasso=bootstrap_lassofit %>% collect_metrics(summarize = FALSE) %>% 
select(id,.metric,.estimate) %>% mutate(model="lasso")
performanceridge=bootstrap_ridgefit %>% collect_metrics(summarize = FALSE) %>% 
select(id,.metric,.estimate) %>% mutate(model="ridge")
performancelm=bootstrap_lmfit %>% collect_metrics(summarize = FALSE) %>% 
select(id,.metric,.estimate) %>% mutate(model="lm")
performancerf=bootstrap_rffit %>% collect_metrics(summarize = FALSE) %>% 
select(id,.metric,.estimate) %>% mutate(model="rf")

performance_all=bind_rows(performancelm,performancelasso,performanceridge,
                          performanceknn,performancerf) %>%
  mutate(model=as.factor(model))

performance_all %>% filter(.metric=="rmse") %>% lm(.estimate~model,data=.) %>% 
anova() %>% tidy()
## since P-value is smaller than 0.05, we reject the hypothesis that all 5
## models have the same mean.
performance_all %>% filter(.metric=="rmse") %>% 
filter(model %in% c("lasso","ridge","lm")) %>%  lm(.estimate~model,data=.) %>%
anova() %>% tidy()
## since P-value is larger than 0.05, we fail to reject the hypothesis that 3
## models have the same mean, which means lm,lasso and ridge model have same 
## performance.
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##applying the bootstrap to parameter estimates
set.seed(1)
control_alz=control_resamples(extract = tidy)
boostlassofit=final_lasso %>% 
fit_resamples(alz_bootstrap,control =control_alz )
boostlasso_coef=boostlassofit %>% select(id,.extracts) %>% unnest(.extracts) %>% 
unnest(.extracts)
boostlasso_coef %>% group_by(term) %>% summarise(mean_estimate=mean(estimate),
                                                 sde=sd(estimate))

boostlmfit=recipeworkflowlm %>% fit_resamples(alz_bootstrap,control=control_alz)
boostlm_coef=boostlmfit %>% select(id,.extracts) %>% unnest(.extracts) %>% 
unnest(.extracts)
boostlm_coef %>% group_by(term) %>% summarise(mean_estimate=mean(estimate),
                                                 sde=sd(estimate))
## lm and lasso regression model have similar estimate. 
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##PCA??
library(broom)
alzpca=alzdata  %>% step_pca(all_numeric(),num_comp = 7)
alzpcaest=prep(alzpca,training = alz_lifeexp)
pcadata=bake(alzpcaest,alz_lifeexp)
a=broom::tidy(pcadata)
b=a %>% mutate(percent=round(a$sd^2/sum(a$sd^2),5)) %>% rename(PC=column)
b %>% ggplot(aes(x = PC, y = percent)) + 
  geom_segment(aes(xend = PC), yend = 0, linewidth = 4)

missna=drop_na(alz_lifeexp)
meanSES=round(mean(missna$SES))
meanMMSE=round(mean(missna$MMSE))
alz_lifeexp=alz_lifeexp %>% mutate_at("SES",~replace_na(.,meanSES))
alz_lifeexp=alz_lifeexp %>% mutate_at("MMSE",~replace_na(.,meanMMSE))
pca=alz_lifeexp %>% select(Age,EDUC,SES,MMSE,CDR,eTIV,nWBV,ASF) %>%  prcomp()
variation=round(pca$sdev^2/sum(pca$sdev^2),4)
pcs=broom::tidy(pca,matrix="pcs")
pcs %>% ggplot(aes(x=PC,y=percent))+geom_segment(aes(xend=PC),yend=0,linewidth=4)

##PC1 almost get 99 percent.
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
##patient level function
patientCDRlevel=function(MRIid){
  CDR=alz_lifeexp %>% filter(`MRI ID`== MRIid) %>% select(CDR)
  if(CDR==0){
    print("the patient is normal")
  }
  if(CDR==0.5){
    print("the patient is very mild dementia")
  }
  if(CDR==1){
    print("the patient is mild dementia")
  }
  if(CDR==2){
    print("the patient is moderate dementia")}
}
```

```{r, echo=TRUE,warning=FALSE,message=FALSE}
#library(usethis)
#use_git_config(user.name="endkg",user.email="chon.iao@uconn.edu")
##gitcreds::gitcreds_set()
## my final project git link: https://github.com/endkg/stat-5125-project


```

## Lastly, base on the data analysis and fit the different model, we can conclude that lm,lasso and ridge regression model have similar performance. But lm's performance is little bit better. 